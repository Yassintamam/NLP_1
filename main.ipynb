{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sst\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "import numpy as np\n",
    "\n",
    "def train_naive_bayes(D, C):\n",
    "    Ndoc = len(D)\n",
    "    logprior = {}\n",
    "    loglikelihood = {}\n",
    "    V = set()\n",
    "    bigdoc = {c: [] for c in C}\n",
    "\n",
    "    # Calculate logprior\n",
    "    for c in C:\n",
    "     Ne = sum(1 for d in D if d[1] == c)\n",
    "     logprior[c] = np.log(Ne / Ndoc)\n",
    "     bigdoc[c] = [d[0] for d in D if d[1] == c]\n",
    "     # Flatten the list of lists\n",
    "     flattened_tokens = [token for sublist in bigdoc[c] for token in\n",
    "      sublist]\n",
    "     V.update(set(flattened_tokens))\n",
    "\n",
    "    # Calculate loglikelihood\n",
    "    for w in V:\n",
    "        for c in C:\n",
    "            count_w_c = sum(d.count(w) for d in bigdoc[c])\n",
    "            loglikelihood[(w, c)] = np.log((count_w_c + 1) / (sum(bigdoc[c].count(w) for w in V) + len(V)))\n",
    "\n",
    "    return logprior, loglikelihood, V\n",
    "\n",
    "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
    "    sum_scores = {c: logprior[c] for c in C}\n",
    "    for word in testdoc:\n",
    "        if word in V:\n",
    "            for c in C:\n",
    "                sum_scores[c] += loglikelihood.get((word, c), 0)\n",
    "    return max(sum_scores, key=sum_scores.get)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"Very Negative\",\n",
    "    1: \"Negative\",\n",
    "    2: \"Neutral\",\n",
    "    3: \"Positive\",\n",
    "    4: \"Very Positive\",\n",
    "}\n",
    "def map_dataset(data):\n",
    "    if data <= 0.2:\n",
    "        return 0  # very negative\n",
    "    elif data <= 0.4:\n",
    "        return 1  # negative\n",
    "    elif data <= 0.6:\n",
    "        return 2  # neutral\n",
    "    elif data <= 0.8:\n",
    "        return 3  # positive\n",
    "    else:\n",
    "        return 4  # very positive\n",
    "\n",
    "def map_to_label(data):\n",
    "    return classes[data]\n",
    "\n",
    "def data_to_tokens(data):\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        tokens = entry['tokens'].split(\"|\")\n",
    "        label = map_dataset(entry['label'])\n",
    "        documents.append((tokens, label))\n",
    "    return documents\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m random\n\u001B[0;32m      3\u001B[0m train_documents \u001B[38;5;241m=\u001B[39m data_to_tokens(dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 4\u001B[0m logprior, loglikelihood, vocab_list \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_naive_bayes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Preprocess the test dataset\u001B[39;00m\n\u001B[0;32m      7\u001B[0m test_documents \u001B[38;5;241m=\u001B[39m data_to_tokens(dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[1;32mIn[27], line 25\u001B[0m, in \u001B[0;36mtrain_naive_bayes\u001B[1;34m(D, C)\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m C:\n\u001B[0;32m     24\u001B[0m         count_w_c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(d\u001B[38;5;241m.\u001B[39mcount(w) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m bigdoc[c])\n\u001B[1;32m---> 25\u001B[0m         loglikelihood[(w, c)] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog((count_w_c \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbigdoc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mc\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mV\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(V)))\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logprior, loglikelihood, V\n",
      "Cell \u001B[1;32mIn[27], line 25\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m C:\n\u001B[0;32m     24\u001B[0m         count_w_c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(d\u001B[38;5;241m.\u001B[39mcount(w) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m bigdoc[c])\n\u001B[1;32m---> 25\u001B[0m         loglikelihood[(w, c)] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog((count_w_c \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28msum\u001B[39m(bigdoc[c]\u001B[38;5;241m.\u001B[39mcount(w) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m V) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(V)))\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logprior, loglikelihood, V\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "train_documents = data_to_tokens(dataset['train'])\n",
    "logprior, loglikelihood, vocab_list = train_naive_bayes(train_documents, [0, 1, 2, 3, 4])\n",
    "\n",
    "# Preprocess the test dataset\n",
    "test_documents = data_to_tokens(dataset['test'])\n",
    "\n",
    "# Define a function to classify a single document\n",
    "def classify_document(document):\n",
    "    return test_naive_bayes(document, logprior, loglikelihood, [0, 1, 2, 3, 4], vocab_list)\n",
    "\n",
    "# Test the classifier on the test dataset\n",
    "predicted_classes = []\n",
    "actual_classes = []\n",
    "\n",
    "for doc_tokens, actual_class in test_documents:\n",
    "    predicted_class = classify_document(doc_tokens)\n",
    "    predicted_classes.append(predicted_class)\n",
    "    actual_classes.append(actual_class)\n",
    "\n",
    "# Compare the predicted classes with the actual classes\n",
    "correct_predictions = sum(1 for pred, actual in zip(predicted_classes, actual_classes) if pred == actual)\n",
    "total_predictions = len(predicted_classes)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "import random\n",
    "# Generate a random phrase and predict its class\n",
    "random_phrase = \"This movie was fantastic! I loved every minute of it.\"\n",
    "random_score = random.uniform(0, 1)\n",
    "predicted_class = classify_document(random_phrase)\n",
    "actual_class = map_dataset(random_score)  # Generating a random sentiment score and mapping it\n",
    "print(\"Random phrase:\", random_phrase)\n",
    "print(\"Predicted class:\", classes[predicted_class])\n",
    "print(\"Actual class:\", classes[actual_class])\n",
    "# Test the classifier on a sample document\n",
    "sample_document = \"This movie was really good. I enjoyed it a lot.\"\n",
    "predicted_class = classify_document(sample_document)\n",
    "print(\"Predicted class:\", classes[predicted_class])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
